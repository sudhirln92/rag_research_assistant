{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30129fa5",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a988672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory : /home/sudhir/DataScience/genai-2025/RAG_research_assistant\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(f\"Current directory : {CURRENT_DIR}\")\n",
    "DB_DIR = os.path.join(CURRENT_DIR, \"db\")\n",
    "\n",
    "STORE_NAME = \"faiss_store\"\n",
    "FILES_PATH = os.path.join(CURRENT_DIR,'docs')\n",
    "PERSIST_DIR = os.path.join(DB_DIR,STORE_NAME)\n",
    "\n",
    "# Choose a small HF model for quick embedding\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ffc137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faiss_store'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSIST_DIR.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4220f9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faiss_store'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f80fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(files_path):\n",
    "\n",
    "    print(\"\\n--- Load Documents ---\")\n",
    "    if not os.path.exists(files_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"The directory {files_path} does not exist\"\n",
    "        )\n",
    "    \n",
    "    # List all pdf files in the directory\n",
    "    pdf_files = [f for f in os.listdir(files_path) if f.endswith(\".pdf\")]\n",
    "    print(f\"List of pdf files: {pdf_files}\")\n",
    "\n",
    "    # Read the text content from each file and store it with metadata\n",
    "    documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(files_path, pdf_file)\n",
    "        # load docs\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            # Add metadata to each document indicating its source\n",
    "            doc.metadata = {\"source\": pdf_file}\n",
    "            documents.append(doc)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    # Display information about the split documents\n",
    "    print(\"\\n--- Document Chunks Information ---\")\n",
    "    print(f\"Number of document chunks: {len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffda6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Load Documents ---\n",
      "List of pdf files: ['GPT3-2005.14165v4.pdf', 'AttentionAllYouNeed-1706.03762v7.pdf']\n",
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 369\n"
     ]
    }
   ],
   "source": [
    "docs = load_document(FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356b742",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed3623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model_name):\n",
    "    \"\"\"Return HuggingFace embeddings instance.\"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "def load_or_create_faiss(documents,store_name,db_dir,model_name):\n",
    "    \"\"\"Create a new FAISS store or update an existing one.\"\"\"\n",
    "    \n",
    "    embeddings = get_embeddings(model_name)\n",
    "\n",
    "    PERSIST_DIR = os.path.join(db_dir,store_name)\n",
    "\n",
    "    if os.path.exists(PERSIST_DIR):\n",
    "        print(\"Existing FAISS DB found. Loading...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            PERSIST_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"Adding new documents to existing DB...\")\n",
    "        vectorstore.add_documents(documents)\n",
    "        vectorstore.save_local(PERSIST_DIR)\n",
    "\n",
    "    else:\n",
    "        print(\"Creating new FAISS DB...\")\n",
    "        vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        vectorstore.save_local(PERSIST_DIR)\n",
    "\n",
    "    print(\" Vector store is ready!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce70757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing FAISS DB found. Loading...\n",
      "Adding new documents to existing DB...\n",
      " Vector store is ready!\n"
     ]
    }
   ],
   "source": [
    "vectorstore = load_or_create_faiss(docs,STORE_NAME,DB_DIR,EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816871f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sudhir/DataScience/genai-2025/RAG_research_assistant/db'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    if os.path.exists(PERSIST_DIR):\n",
    "        return FAISS.load_local(\n",
    "            PERSIST_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    return None\n",
    "\n",
    "def file_exists_in_db(vectorstore, filename):\n",
    "    \"\"\"Check metadata in DB to see if filename already exists.\"\"\"\n",
    "    if vectorstore is None:\n",
    "        return False\n",
    "\n",
    "    # metadata is stored inside docstore\n",
    "    for _id, doc in vectorstore.docstore._dict.items():\n",
    "        if doc.metadata.get(\"source\") == filename:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a367cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorstore_retriver(store_name,db_dir,model_name):\n",
    "    \"\"\"Get vector store retriver \"\"\"\n",
    "    embeddings = get_embeddings(model_name)\n",
    "\n",
    "    PERSIST_DIR = os.path.join(db_dir,store_name)\n",
    "\n",
    "    if os.path.exists(PERSIST_DIR):\n",
    "        vectorstore = FAISS.load_local(\n",
    "            PERSIST_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        return vectorstore\n",
    "    else:\n",
    "        raise FileNotFoundError(f\" The vector store {store_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da87423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = vectorstore_retriver(STORE_NAME,DB_DIR,EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc291906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='765026b2-2213-4331-a97b-c88410f482f5', metadata={'source': 'GPT3-2005.14165v4.pdf'}, page_content='Winogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0%\\nTable C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it\\nhas a single N-gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the\\npercent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows\\nthe number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we\\nuse the metric speciﬁed in “Metric”. These scores come from evaluations with a different seed for the random examples\\nused for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.\\n45'),\n",
       "  1.5261037),\n",
       " (Document(id='80d279d9-d981-4682-81fc-74720132ad66', metadata={'source': 'GPT3-2005.14165v4.pdf'}, page_content='Winogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0%\\nTable C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it\\nhas a single N-gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the\\npercent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows\\nthe number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we\\nuse the metric speciﬁed in “Metric”. These scores come from evaluations with a different seed for the random examples\\nused for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.\\n45'),\n",
       "  1.5261037),\n",
       " (Document(id='3fdec36d-aa83-4d55-9b08-dc879153e35e', metadata={'source': 'GPT3-2005.14165v4.pdf'}, page_content='whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic\\ntasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data\\nmarked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test\\ncontamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps\\nbetween test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl\\ndocuments per Section 2.2.\\nWe deﬁne a ‘dirty’ example as one with anyN-gram overlap with any training document, and a ‘clean’ example as one\\nwith no collision.\\nTest and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed\\nby this analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it'),\n",
       "  1.6284775),\n",
       " (Document(id='bf77c918-9bf8-4ce4-a5f4-92a64948d66d', metadata={'source': 'GPT3-2005.14165v4.pdf'}, page_content='whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic\\ntasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data\\nmarked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test\\ncontamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps\\nbetween test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl\\ndocuments per Section 2.2.\\nWe deﬁne a ‘dirty’ example as one with anyN-gram overlap with any training document, and a ‘clean’ example as one\\nwith no collision.\\nTest and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed\\nby this analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it'),\n",
       "  1.6284775)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score('what is rag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac79613",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdd1e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    do_sample=False,\n",
    "    provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c87ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from langchain_core.retrievers import RetrievalQA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/langchain/chains/__init__.py:93\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/langchain/_api/module_import.py:69\u001b[39m, in \u001b[36mcreate_importer.<locals>.import_by_name\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m     64\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImporting from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not allowed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAllowed top-level packages are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWED_TOP_LEVEL_PKGS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_module.startswith(\u001b[33m\"\u001b[39m\u001b[33mlangchain_community\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStore\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, Field, model_validator\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chain\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseCombineDocumentsChain\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstuff\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StuffDocumentsChain\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/langchain/chains/base.py:21\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     AsyncCallbackManager,\n\u001b[32m     15\u001b[39m     AsyncCallbackManagerForChainRun,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     Callbacks,\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMemory\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunInfo\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     RunnableConfig,\n\u001b[32m     25\u001b[39m     RunnableSerializable,\n\u001b[32m     26\u001b[39m     ensure_config,\n\u001b[32m     27\u001b[39m     run_in_executor,\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.memory'"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "# from langchain_core.retrievers import RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fa51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a73f2e112141438491765ab07b458a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7703697fefbb47f68fecb74f029fd8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cee36a0a7684d48a78dc21ca1614623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c391c38363b4b79bb1c63a0257fa223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f25703d1da4f028bdeb7d49dba4522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd1972c98a0436a9c927d7ba32ea206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3ed3c085d14847a28e675b93ca5fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce17a7958840ae81942d03083af8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed869f0a0d9465795aa58c60b6b7f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f063b2d62324f5e907bf9ed0a079d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89d6048e4d44d08a3691c7201ecf59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfc11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.faiss.FAISS"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create and persist vector store\n",
    "def create_vector_store(docs, embeddings, store_name,db_dir):\n",
    "    persistent_directory = os.path.join(db_dir,store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"Creating vector store: {store_name}\")\n",
    "        Chroma.from_documents(docs,embeddings,persist_directory=persistent_directory)\n",
    "        print(f\"Finished creating vector store: {store_name}\")\n",
    "    else:\n",
    "        print(f\"Appending document to vector store: {store_name}\")\n",
    "        Chroma.add_documents(docs,embeddings)\n",
    "        print(f\"Finished creating vector store: {store_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85214899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
    "len(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412acbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = base_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81939c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are a research assistant. \n",
      "\n",
      "### Context:\n",
      "arXiv:2506.06962v3  [cs.CV]  14 Jun 2025\n",
      "AR-RAG: Autoregressive Retrieval Augmentation for\n",
      "Image Generation\n",
      "Jingyuan Qi* 1\n",
      "Zhiyang Xu* 1\n",
      "Qifan Wang2\n",
      "Lifu Huang3\n",
      "1Virginia Tech\n",
      "2Meta\n",
      "3 UC Davis\n",
      "jingyq1@vt.edu\n",
      "(a) Vanilla Image Generation\n",
      "Prompt\n",
      "(c) Patch-based Autoregressive Retrieval Augmentation (Ours)\n",
      "...\n",
      "Augmentation\n",
      "Generation\n",
      "Prompt\n",
      "Generated Image\n",
      "Generated Image\n",
      "(b) Image-Based Retrieval Augmentation\n",
      "Prompt\n",
      "Generated Image\n",
      "Retrieved Images\n",
      "...\n",
      "Query\n",
      "Key\n",
      "Query\n",
      "Key\n",
      "Query\n",
      "Value\n",
      "Key\n",
      "Retrieval\n",
      "Value\n",
      "Value\n",
      "?\n",
      "?\n",
      "Next Image Patch\n",
      "?\n",
      "Figure 1: Comparison between Autoregressive Retrieval Augmentation (AR-RAG) for image\n",
      "generation in (c) and existing image generation paradigms in (a) (b). In AR-RAG, image patches in\n",
      "red boxes denote retrieval queries and keys, image patches in blue boxes are retrieved values, and\n",
      "gray boxes with the question mark are next image patches to be predicted. (Caption: A white cat is\n",
      "playing basketball on the court.)\n",
      "Abstract\n",
      "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel\n",
      "paradigm that enhances image generation by autoregressively incorporating k-\n",
      "nearest neighbor retrievals at the patch level. Unlike prior methods that perform\n",
      "a single, static retrieval before generation and condition the entire generation on\n",
      "fixed reference images, AR-RAG performs context-aware retrievals at each gen-\n",
      "eration step, using prior-generated patches as queries to retrieve and incorporate\n",
      "the most relevant patch-level visual references, enabling the model to respond to\n",
      "evolving generation needs while avoiding limitations (e.g., over-copying, stylis-\n",
      "tic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose\n",
      "two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD),\n",
      "a training-free plug-and-use decoding strategy that directly merges the distribu-\n",
      "tion of model-predicted patches with the distribution of retrieved patches, and\n",
      "(2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning\n",
      "method that progressively smooths the features of retrieved patches via multi-scale\n",
      "convolution operations and leverages them to augment the image generation pro-\n",
      "cess. We validate the effectiveness of AR-RAG on widely adopted benchmarks,\n",
      "1Jingyuan Qi and Zhiyang Xu contributed equally to this work.\n",
      "Preprint. Under review.\n",
      "including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant\n",
      "performance gains over state-of-the-art image generation models.1\n",
      "1\n",
      "Introduction\n",
      "Recent advancements in image generation have demonstrated remarkable capabilities in producing\n",
      "photorealistic images based on user prompts [31, 28, 7, 37, 10, 41, 9, 43, 45, 27, 6]. However,\n",
      "despite these improvements, the generated images often exhibit local distortions and inconsistencies,\n",
      "particularly in visual objects that possess complex structures [11], frequently interact with other\n",
      "objects and the surrounding scene [22, 26], or are underrepresented in the training data [8]. A\n",
      "promising approach to mitigating these challenges is retrieval-augmented generation (RAG), which\n",
      "enhances the generation process by incorporating real-world images as additional references [8, 3].\n",
      "While RAG has been extensively explored in the language domain [23, 13], its application to image\n",
      "and multimodal generation remains largely underdeveloped. A few existing studies [3, 8, 46, 48, 49]\n",
      "bridge this gap by performing a single-step retrieval based on the input prompt prior to generation,\n",
      "conditioning the entire image generation process on fixed visual cues (Figure 1 (b)). However, as\n",
      "demonstrated in our pilot study (Section 5.2), such static, coarse-grained retrieval approaches [3, 8, 49]\n",
      "frequently introduce irrelevant or weakly aligned visual contents that persist throughout generation.\n",
      "Since the retrieved images are selected once, before decoding begins, and remain unchanged, these\n",
      "methods cannot respond to the evolving generation needs, resulting in over-copying of irrelevant\n",
      "details, stylistic bias, and the hallucination of unrelated visual elements. For example, as shown in\n",
      "Figure 1(b), a basketball player present in the retrieved references, despite being irrelevant to the\n",
      "input prompt, unintentionally appears in the generated image.\n",
      "In this paper, we propose Autoregressive Retrieval Augmentation (AR-RAG), a novel retrieval-\n",
      "augmented paradigm for image generation that dynamically and autoregressively incorporates patch-\n",
      "level k-nearest-neighbor (k-NN) retrievals throughout the generation process (Figure 1(c)). In contrast\n",
      "to prior methods that rely on static, coarse-grained retrievals of entire reference images, typically\n",
      "using captions as retrieval queries and keys, AR-RAG performs fine-grained, step-wise retrieval\n",
      "at the image patch level. Specifically, as generation unfolds, AR-RAG leverages the already-\n",
      "generated surrounding patches as localized queries to retrieve contextually similar patches from\n",
      "a pre-constructed patch-level database. This database is built by encoding real-world images into\n",
      "latent patch features, where each entry contains a patch embedding as a value and the embeddings\n",
      "of its h-hop spatial neighbors as a key. During the generation of the next target patch (gray boxes\n",
      "in Figure 1(c)), AR-RAG retrieves the top-K most relevant patches (blue boxes) by measuring\n",
      "similarity between the surrounding generated context patches (red boxes) and database keys (also\n",
      "red boxes). These retrieved patches are then integrated into the model to inform and enhance the\n",
      "prediction of the next patch, enabling the model to dynamically adjust to local generation needs.\n",
      "By conditioning on the evolving generation context as retrieval queries, AR-RAG ensures that\n",
      "retrieved visual references remain relevant throughout the generation process, encouraging local\n",
      "semantic coherence. Moreover, the patch-level retrieval allows for precise integration of visual\n",
      "elements without overcommitting to entire reference images, avoiding the limitations of over-copying\n",
      "or irrelevant conditioning observed in static retrieval.\n",
      "To realize the AR-RAG framework, we introduce two parallel implementations: (1) Distribution-\n",
      "Augmentation in Decoding (DAiD), a training-free, plug-and-play decoding strategy that merges\n",
      "the model’s predicted patch distribution with that of the retrieved patches. Specifically, the top-K\n",
      "retrieved patches are assigned probabilities inversely proportional to their normalized ℓ2 distances\n",
      "computed from the query and key patch embeddings. These probabilities are then linearly combined\n",
      "with the model’s native output distribution to guide the next patch prediction, enabling retrieval-aware\n",
      "generation without any additional training. (2) Feature-Augmentation in Decoding (FAiD), a\n",
      "parameter-efficient fine-tuning approach that integrates retrieved patches into the generation process\n",
      "through learned smoothing and blending mechanisms. Specifically, when generating the next image\n",
      "token, FAiD operates in two stages: (1) refining the retrieved patch features by adjusting them to\n",
      "better fit the local context of the already generated surrounding patches, based on parameterized\n",
      "convolutional operations of varying kernel sizes; and (2) blending the refined features of retrieved\n",
      "patches with the model’s predicted feature representation for the next patch, based on compatibility\n",
      "1Code and model checkpoints can be found at https://github.com/PLUM-Lab/AR-RAG.\n",
      "2\n",
      "scores computed for each retrieved patch to quantify their alignment with the current generation\n",
      "context. To enable iterative refinement, we insert multiple FAiD modules at selected transformer\n",
      "layers, where the output of each FAiD module, i.e., the context-aware retrieved features blended\n",
      "at that layer, is forwarded as input to the next FAiD module in deeper layers. This progressive\n",
      "retrieval refinement mechanism allows the model to incrementally enhance its predictions as patch-\n",
      "level representations evolve through the network. We evaluate AR-RAG on three widely adopted\n",
      "benchmarks, including Midjourney-30K 2, Geneval [14], and DPG-Bench [18]. Experimental results\n",
      "demonstrate that both DAiD and FAiD significantly improve the coherence and naturalness of\n",
      "generated images while introducing only marginal computational overhead.\n",
      "The contributions of our work can be summarized as follows:\n",
      "• We propose AR-RAG, the first patch-level autoregressive retrieval augmentation framework which\n",
      "dynamically retrieves and integrates fine-grained visual content to enhance image generation,\n",
      "while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing image-level\n",
      "retrieval augmentation methods.\n",
      "• We introduce Distribution-Augmentation in Decoding (DAiD), a training-free, plug-and-play\n",
      "decoding strategy that directly integrates the distribution of retrieved patches into that predicted by\n",
      "the image generation models, enabling easy integration into existing architectures.\n",
      "• We introduce Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning frame-\n",
      "work that progressively refines and blends retrieval signals via lightweight convolutional modules,\n",
      "enhancing spatial coherence and visual quality across layers.\n",
      "• Extensive experiments and analysis show that AR-RAG significantly improves performance of\n",
      "state-of-the-art image generation model across diverse metrics. In particular, Janus-Pro with FAiD\n",
      "achieves 6.67 FID on Midjourney-30K and 0.78 overall score on GenEval, establishing a new state\n",
      "of the art among autoregressive image generation models of comparable scale.\n",
      "2\n",
      "Preliminary\n",
      "Autoregressive Image Generation Models\n",
      "We implement both DAiD and FAiD based on Janus-\n",
      "Pro [9], an autoregressive (AR) unified generation model, due to its strong performance. Janus-Pro is\n",
      "initialized from a transformer-based pre-trained large-language model [2], and employs a quantized\n",
      "autoencoder [37] to encode images into discrete image tokens. During multimodal pretraining, the\n",
      "model learns to predict a sequence of discrete image tokens [v1, v2, ...vN] conditioned on an input\n",
      "text prompt [t1, t2, ...tM]. The training objective is formally defined as:\n",
      "arg max\n",
      "ϕ\n",
      "D\n",
      "X\n",
      "N\n",
      "X\n",
      "n=1\n",
      "Pϕ(vn|t1, t2, ..., tM, v1, ...vn−1)\n",
      "(1)\n",
      "where D is the training corpus. This is the same training objective used in our FAiD method in\n",
      "Section 3.3. We argue that DAiD and FAiD can be extended to any image generation model that\n",
      "autoregressively predicts probability distributions of discrete image tokens such as LlamaGen [37],\n",
      "Show-o [44] and VAR [38].\n",
      "Quantized Autoencoder\n",
      "The quantized autoencoder used in Janus-Pro consists of an encoder θenc,\n",
      "a decoder θdec, and a codebook Z. The encoder, a convolutional neural network, downsamples and\n",
      "compresses raw pixel inputs into compact patch representations. During the quantization process, each\n",
      "patch representation is mapped to an index in the codebook by identifying its nearest neighbor vector\n",
      "in the codebook. In the decoding stage, these patch indices are mapped back to their corresponding\n",
      "vector representations via the codebook, and the decoder, another convolutional neural network,\n",
      "reconstructs the image from these compact representations. In our implementation, we leverage this\n",
      "autoencoder to build the coupled database for Janus-pro which is detailed in Section 3.1.\n",
      "3\n",
      "AR-RAG: Patch-based Autoregressive Retrieval Augmentation\n",
      "3.1\n",
      "Patch-based Retrieval Database Construction\n",
      "We build a patch-based retrieval database based on several large-scale, real-world image datasets,\n",
      "including CC12M [5] and JourneyDB [36]. Specifically, for each image I, we encode it into N\n",
      "2https://huggingface.co/datasets/playgroundai/MJHQ-30K\n",
      "3\n",
      "Patch-based Retrieval\n",
      "h-hop surrounding\n",
      "patches\n",
      "AR Model\n",
      "Dmodel\n",
      "Distribution-Augmentation in Decoding (DAiD)\n",
      "Generated Next \n",
      "Image Patches\n",
      "Dmerge\n",
      "？\n",
      "Generated Patches\n",
      "DRetrieval\n",
      "Figure 2: The decoding process in Distribution-Augmentation in Decoding (DAiD).\n",
      "patches using the quantized autoencoder [37], θEnc, from Janus-Pro: V = θenc(I) ∈R\n",
      "√\n",
      "N×\n",
      "√\n",
      "N×d,\n",
      "where d is the hidden dimension, and Vij corresponds to the latent representation of the patch at\n",
      "position (i, j). We utilize each patch vector Vij as the value of a database entry and the representation\n",
      "of its h-hop surrounding patches as the key. Here, the h-hop surrounding patch representation is\n",
      "formed by concatenating the vectors of adjacent patches centering around (i, j) in a top-to-bottom,\n",
      "left-to-right order. For example, for a patch at position (i, j), the 1-hop surrounding representation\n",
      "spans 8 surrounding patches [V(i−1)(j−1) : V(i−1)(j) : V(i−1)(j+1) : V(i)(j−1) : V(i)(j+1) :\n",
      "V(i+1)(j−1) : V(i+1)(j) : V(i+1)(j+1)] where : denotes the concatenation operation of image patch\n",
      "features. If a patch is located at the edge of the image and lacks certain surrounding patches, we\n",
      "substitute each missing surrounding patch with a zero vector 0.\n",
      "3.2\n",
      "Distribution-Augmentation in Decoding (DAiD)\n",
      "Given a text prompt T, Janus-Pro autoregressively predicts a sequence of image tokens [v1, v2, ...vN]\n",
      "where per-token probability is defined in Equation 1. As shown in Figure 2, DAiD augments this\n",
      "process by incorporating probability distributions from retrieved image patches. Specifically, when\n",
      "Janus-Pro predicts the next image token vij, we first utilize the codebook Z to convert vij’s h-\n",
      "hop already generated surrounding patches into patch representations. If no surrounding image\n",
      "tokens are available at a given position (e.g., when i = 0 or j = 0), we use the zero vector 0 as a\n",
      "placeholder. Once we compute the representation of vij’s h-hop surrounding patches, we leverage it\n",
      "as the retrieval query and retrieve the top-K most similar patch representations from the database\n",
      "constructed in Section 3.1 using l2 distance. We denote the representations of the top-K retrieved\n",
      "patches as [ˆv1, ˆv2, ..., ˆvK] and their corresponding l2 distances as [s1, s2, ..., sK]. These retrieved\n",
      "representations are then mapped back to discrete token indices using the codebook: ˆvk = Z(ˆvk).\n",
      "To augment the generation process with the retrieved image tokens [ˆv1, ˆv2, ..., ˆvK], we create a\n",
      "retrieval-based distribution Dretrieval ∈R|Z| over the entire codebook Z, where |Z| is the codebook\n",
      "size. Tokens not included in the top-K retrieved set are assigned a probability of 0. For tokens within\n",
      "the top-K, we compute their probabilities using a softmax over their l2 distance to the query, scaled\n",
      "by a retrieval temperature hyperparameter τ:\n",
      "Dretrieval[v] =\n",
      "\u001ap(ˆvk)\n",
      "if v = ˆvk for some m ∈{1, 2, ..., K}\n",
      "0\n",
      "otherwise\n",
      "(2)\n",
      "p(ˆvk) =\n",
      "exp(−sk/τ)\n",
      "PK\n",
      "k=1 exp(−sk/τ)\n",
      ",\n",
      "(3)\n",
      "This creates a sparse distribution where only the top-K retrieved tokens have non-zero probabilities.\n",
      "Finally, we merge this retrieval distribution with the model’s predicted distribution Dmodel using a\n",
      "weighted average:\n",
      "Dmerge = (1 −λ) · Dmodel + λ · Dretrieval,\n",
      "(4)\n",
      "where λ ∈[0, 1] is the retrieval weight hyperparameter controlling the influence of retrieved patches\n",
      "on the final distribution. The next token is then sampled from this merged distribution: vij ∼Dmerge.\n",
      "3.3\n",
      "Feature-Augmentation in Decoding (FAiD)\n",
      "While DAiD offers a training free approach to directly augment the probability distribution of\n",
      "predicted patches using retrieved ones, it suffers from noise propagation and limited flexibility in\n",
      "4\n",
      "？\n",
      "Retrieval Database\n",
      "...\n",
      "Patch-based Image Retriever\n",
      "Generated Patches\n",
      "Convolutional Layer\n",
      "MLP Layer\n",
      "Feature-Augmentation in Decoding (FAiD)\n",
      "Decoder\n",
      "Layer\n",
      "RMS Norm\n",
      "Self-Attn\n",
      "RMS Norm\n",
      "FFD\n",
      "RMS Norm\n",
      "Previous Patch Embeddings\n",
      "Retrieval Embeddings\n",
      "Janus-Pro\n",
      "SPB\n",
      "...\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "1\n",
      "2\n",
      "10\n",
      "1\n",
      "2\n",
      "9\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Figure 3: Overall architecture of Feature-Augmentation in Decoding (FAiD).\n",
      "fully leveraging the fine-grained visual information in the retrieved patches. We thus further propose\n",
      "FAiD, a feature-based autoregressive augmentation strategy to enhance the image generation process.\n",
      "As illustrated in Figure 3, when predicting the next token vij during image generation, we employ\n",
      "the same retrieval process described in Section 3.2 to obtain the top-K most relevant patches and\n",
      "their representations [ˆv1, ˆv2, ..., ˆvK] from our database. To effectively incorporate them into the\n",
      "autoregressive generation process, FAiD consists of two steps: (1) refining retrieved patches to ensure\n",
      "coherence with the surrounding context of vij in the generated image, and (2) adaptively blending the\n",
      "representation of refined patches with the hidden state of the predicted next patch based on learned\n",
      "compatibility scores. To enable progressive refinement of retrieved information as representations\n",
      "evolve through the network, we insert a FAiD module for every L/b decoder layers of the generation\n",
      "model, where L denotes the total number of decoder layers and b is a hyperparameter.\n",
      "Multi-Scale Feature Smoothing\n",
      "The key of effective patch integration lies in ensuring spatial\n",
      "coherence between retrieved patches and the surrounding image context. To achieve this, we propose\n",
      "multi-scale feature smoothing (Algorithm 1 in Appendix A), where multi-scale convolutions are\n",
      "applied to retrieved patches within the generation context, so that the retrieved visual features are\n",
      "smoothed to preserve structural and stylistic consistency with the surrounding context of the predicted\n",
      "token. Specifically, at each step when predicting the next image token vij, we first construct a 2D\n",
      "spatial representation Hl ∈R\n",
      "√\n",
      "N×\n",
      "√\n",
      "N×D of the current partially-generated image by arranging\n",
      "their hidden states [hl\n",
      "1, hl\n",
      "2, ..., hl\n",
      "n−1, hl\n",
      "n] from the current decoder layer l. We use 0 vectors as\n",
      "placeholders for positions that have not yet been generated. Then, we transform the retrieved patch\n",
      "representations [ˆv1, ˆv2, ..., ˆvK] into the generation model’s hidden space by mapping each patch\n",
      "ˆvk to a discrete token index via the codebook Z and embedding it through the pretrained image\n",
      "embedding layer Embimg:\n",
      "[ˆh1, ˆh2, ..., ˆhK] = Embimg([Z(ˆv1), Z(ˆv2), ..., Z(ˆvK)])\n",
      "(5)\n",
      "For each retrieved patch ˆhk, we create a copy of Hl where position (i, j) (the location of vij) is\n",
      "replaced with ˆhk. We then apply convolution operations at multiple scales (2 × 2 through Q × Q) to\n",
      "capture contextual patterns at different resolutions. To maintain computational efficiency, we only\n",
      "perform convolution operations when the kernel covers position (i, j), rather than processing the\n",
      "entire image. Each convolution kernel Convq×q produces a refined representation ˆhq\n",
      "k for the retrieved\n",
      "patch at scale q. The final refined representation for each retrieved patch is computed as a weighted\n",
      "sum of these multi-scale features:\n",
      "ˆhk ←\n",
      "Q\n",
      "X\n",
      "q=2\n",
      "softmax(Ω)q · ˆhq\n",
      "k\n",
      "(6)\n",
      "where Ω= [ω2, ..., ωQ] are learnable parameters that determine the importance of each scale.\n",
      "5\n",
      "Feature Augmentation\n",
      "After feature smoothing, some of the retrieved patch features may still not\n",
      "be able to fit into the surrounding neighbors and hence we need to lower their impact in the final repre-\n",
      "sentation. Thus, we compute a compatibility score for each of the refined patches. This is achieved by\n",
      "projecting each refined retrieved patch representation through a linear transformation parameterized\n",
      "by a weight matrix W ∈R1×D, yielding the score sk = ˆhkWT . The final representation for the\n",
      "next image token vij after layer j is computed as:\n",
      "h(l+1)\n",
      "ij\n",
      "= hl\n",
      "ij + ∆hl\n",
      "ij +\n",
      "K\n",
      "X\n",
      "k=1\n",
      "skˆhk\n",
      "(7)\n",
      "Here, hl\n",
      "ij is the residual, ∆hl\n",
      "ij is the updated representation from the transformer layer l, and\n",
      "PK\n",
      "k=1 skˆhk is the contribution of the retrieved image patches.\n",
      "4\n",
      "Experiment Setup\n",
      "Patch-based Retrieval Database\n",
      "To construct our patch-level retrieval database, we randomly\n",
      "sample 5.7 million images from CC12M [5], 3.3 million from JourneyDB [36], and 4.6 million from\n",
      "DataComp [12], while ensuring that any samples included in the testing set are excluded to prevent\n",
      "data leakage. Each image is encoded into a sequence of patch-level representations and image tokens\n",
      "using the same image tokenizer employed in the Janus-Pro model. For efficient similarity search, we\n",
      "implement our retriever using the FAISS library [21].\n",
      "Training Setup\n",
      "We adopt Janus-Pro-1B [9] and Show-o [44] as our backbone models and fine-tune\n",
      "them on a dataset of 50,000 image-caption pairs sampled from CC12M [5] and Midjourney-v6 3.\n",
      "We empirically determine the optimal hyperparameters for DAiD and FAiD, and the complete\n",
      "hyperparameter optimization experiment results can be found in Appendix C.2. Further details\n",
      "regarding the training dataset construction and implementation can be found in Appendix B.3.\n",
      "Baselines\n",
      "To evaluate the effectiveness of our proposed methods, we adopt several state-of-the-art\n",
      "image generation approaches as baselines, including non-retrieval models such as LlamaGen [37],\n",
      "LDM [32], Stable Diffusion (SDv1.5 and SDv3) [31, 10], PixArt-alpha [7], DALL-E 2 [30], Show-\n",
      "o[44], and Janus-Pro [9], and image-based retrieval augmentation methods, including RDM [3],\n",
      "RA-CM3 [46], and ImageRAG[33]. Since pretrained models of RA-CM3 are not publicly available,\n",
      "we try our best to replicate their method based on Janus-Pro to ensure a fair comparison. More details\n",
      "of training and implementation of RA-CM3 can be found in Appendix B.1.\n",
      "Evaluation Benchmarks and Metrics\n",
      "To comprehensively evaluate our proposed methods, we\n",
      "employ three benchmarks: (1) GenEval [14], which assesses models’ ability to generate images with\n",
      "specific attributes and relationships described in text prompts; (2) DPG-Bench [18], which evaluates\n",
      "performance on detailed prompts with complex requirements; and (3) Midjourney-30k [40], where we\n",
      "employ three complementary metrics: FID [17] for measuring statistical similarity between generated\n",
      "and real image distributions, CMMD [20] for assessing alignment with human perception using CLIP\n",
      "embeddings, and FWD [39] for evaluating spatial and frequency coherence through wavelet packet\n",
      "coefficients. For all three metrics, lower scores indicate higher quality generated images. Detailed\n",
      "descriptions of these benchmarks and metrics can be found in Appendix B.4.\n",
      "5\n",
      "Results and Discussion\n",
      "5.1\n",
      "Text-to-Image Generation Results\n",
      "Tables 1, 2, and 3 present performance comparisons across multiple benchmarks, where our AR-\n",
      "RAG methods consistently outperform existing approaches. Notably, previous retrieval-augmented\n",
      "approaches such as RDM and ImageRAG perform worse than their non-retrieval counterparts (LDM\n",
      "and SDXL, respectively) on both GenEval and DPG-Bench. We provide detailed analysis for existing\n",
      "image-level retrieval methods and highlight the unique advantages of our AR-RAG frameworks in the\n",
      "following discussion and Section 5.2. Appendix C.1 provides a benchmark analysis to demonstrate\n",
      "the effectiveness of patch-level retrieval in our AR-RAG methods.\n",
      "3https://huggingface.co/datasets/brivangl/midjourney-v6-llava\n",
      "6\n",
      "Method\n",
      "Params\n",
      "Single Obj.\n",
      "Two Obj.\n",
      "Counting\n",
      "Colors\n",
      "Position\n",
      "Color Attri.\n",
      "Overall ↑\n",
      "Non Retrieval-Augmented Model\n",
      "PixArt-α\n",
      "0.6B\n",
      "0.98\n",
      "0.50\n",
      "0.44\n",
      "0.80\n",
      "0.08\n",
      "0.07\n",
      "0.48\n",
      "LlamaGen\n",
      "0.8B\n",
      "0.71\n",
      "0.34\n",
      "0.21\n",
      "0.58\n",
      "0.07\n",
      "0.04\n",
      "0.32\n",
      "SDv1.5\n",
      "0.9B\n",
      "0.97\n",
      "0.38\n",
      "0.35\n",
      "0.76\n",
      "0.04\n",
      "0.06\n",
      "0.43\n",
      "SDv2.1\n",
      "0.9B\n",
      "0.98\n",
      "0.51\n",
      "0.44\n",
      "0.85\n",
      "0.07\n",
      "0.17\n",
      "0.50\n",
      "Janus-Pro\n",
      "1.0B\n",
      "0.98\n",
      "0.77\n",
      "0.52\n",
      "0.84\n",
      "0.61\n",
      "0.55\n",
      "0.71\n",
      "Show-o\n",
      "1.3B\n",
      "0.98\n",
      "0.80\n",
      "0.66\n",
      "0.84\n",
      "0.31\n",
      "0.50\n",
      "0.68\n",
      "LDM\n",
      "1.4B\n",
      "0.92\n",
      "0.29\n",
      "0.23\n",
      "0.7\n",
      "0.02\n",
      "0.05\n",
      "0.37\n",
      "SD3 (d=24)\n",
      "2.0B\n",
      "0.98\n",
      "0.74\n",
      "0.63\n",
      "0.67\n",
      "0.34\n",
      "0.36\n",
      "0.62\n",
      "SDXL\n",
      "2.6B\n",
      "0.98\n",
      "0.74\n",
      "0.39\n",
      "0.85\n",
      "0.15\n",
      "0.23\n",
      "0.55\n",
      "DALL-E 2\n",
      "6.5B\n",
      "0.94\n",
      "0.66\n",
      "0.49\n",
      "0.77\n",
      "0.10\n",
      "0.19\n",
      "0.52\n",
      "DALL-E 3\n",
      "-\n",
      "0.96\n",
      "0.87\n",
      "0.47\n",
      "0.83\n",
      "0.43\n",
      "0.45\n",
      "0.67\n",
      "Transfusion\n",
      "7.3B\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "0.63\n",
      "Chameleon\n",
      "34B\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "0.39\n",
      "Retrieval-Augmented Model\n",
      "RDM\n",
      "1.4B\n",
      "0.91\n",
      "0.21\n",
      "0.28\n",
      "0.71\n",
      "0.02\n",
      "0.04\n",
      "0.36\n",
      "ImageRAG\n",
      "3.5B\n",
      "0.93\n",
      "0.06\n",
      "0.03\n",
      "0.37\n",
      "0.01\n",
      "0.03\n",
      "0.24\n",
      "Janus-Pro\n",
      "+ RA-CM3\n",
      "1.0B\n",
      "0.98\n",
      "0.78\n",
      "0.41\n",
      "0.84\n",
      "0.42\n",
      "0.49\n",
      "0.65 (-0.06)\n",
      "+ DAiD (ours)\n",
      "1.0B\n",
      "0.98\n",
      "0.82\n",
      "0.54\n",
      "0.87\n",
      "0.63\n",
      "0.49\n",
      "0.72 (+0.01)\n",
      "+ FAiD (ours)\n",
      "1.2B\n",
      "1.00\n",
      "0.88\n",
      "0.50\n",
      "0.86\n",
      "0.70\n",
      "0.73\n",
      "0.78 (+0.07)\n",
      "Table 1: Evaluation of text-to-image generation ability on GenEval benchmark. Note our methods\n",
      "are based on Janus-Pro highlighted in gray.\n",
      "Method\n",
      "Params\n",
      "Global\n",
      "Entity\n",
      "Attribute\n",
      "Relation\n",
      "Other\n",
      "Overall ↑\n",
      "Non Retrieval-Augmented Model\n",
      "PixArt-α\n",
      "0.6B\n",
      "74.97\n",
      "97.32\n",
      "78.60\n",
      "82.57\n",
      "76.96\n",
      "71.11\n",
      "SDv1.5\n",
      "0.9B\n",
      "74.63\n",
      "74.23\n",
      "75.39\n",
      "73.49\n",
      "67.81\n",
      "63.18\n",
      "Janus-Pro\n",
      "1.0B\n",
      "81.76\n",
      "84.53\n",
      "84.34\n",
      "92.22\n",
      "75.20\n",
      "77.26\n",
      "Lumina-Next\n",
      "2.0B\n",
      "82.82\n",
      "88.65\n",
      "86.44\n",
      "80.53\n",
      "81.82\n",
      "74.63\n",
      "SDXL\n",
      "3.5B\n",
      "83.27\n",
      "82.43\n",
      "80.91\n",
      "86.76\n",
      "80.41\n",
      "74.65\n",
      "Retrieval-Augmented Model\n",
      "RDM\n",
      "1.4B\n",
      "62.36\n",
      "40.46\n",
      "60.20\n",
      "69.16\n",
      "24.68\n",
      "26.51\n",
      "ImageRAG\n",
      "3.5B\n",
      "61.35\n",
      "32.77\n",
      "53.87\n",
      "60.38\n",
      "18.42\n",
      "19.82\n",
      "Janus-Pro\n",
      "+ RA-CM3\n",
      "1B\n",
      "81.76\n",
      "81.03\n",
      "83.32\n",
      "90.60\n",
      "70.80\n",
      "73.76 (-3.50)\n",
      "+DAiD (ours)\n",
      "1.0B\n",
      "83.58\n",
      "84.46\n",
      "84.76\n",
      "91.49\n",
      "76.40\n",
      "77.88 (+0.62)\n",
      "+FAiD (ours)\n",
      "1.2B\n",
      "82.67\n",
      "85.80\n",
      "85.38\n",
      "92.30\n",
      "76.80\n",
      "79.36 (+2.10)\n",
      "Table 2: Evaluation of text-to-image generation ability on DPG-Bench. Note our methods are based\n",
      "on Janus-Pro highlighted in gray.\n",
      "On GenEval, our methods show significant improvements in categories such as “Two Obj.” and\n",
      "“Position,” which demand accurate multi-object generation and spatial arrangement. These gains are\n",
      "largely due to the local and dynamic nature of our autoregressive patch-level retrieval. Consider the\n",
      "prompt “a green couch and an orange umbrella”, a combination that rarely co-occurs in real-world\n",
      "images. Static full-image retrieval methods may retrieve references containing only one of the objects.\n",
      "Taking these references as a global visual prior throughout the generation can lead the model to\n",
      "overfit to irrelevant layouts or dominant visual structures in the retrieved examples. On DPG-Bench,\n",
      "which features dense and highly detailed prompts, the performance gap between our method and\n",
      "prior retrieval-augmented approaches becomes even more substantial. Similar as GenEval, existing\n",
      "Model\n",
      "Params\n",
      "CMMD ↓\n",
      "FID ↓\n",
      "FWD ↓\n",
      "RDM\n",
      "1.4B\n",
      "0.71\n",
      "19.17\n",
      "34.95\n",
      "ImageRAG\n",
      "3.5B\n",
      "0.32\n",
      "19.39\n",
      "62.65\n",
      "Show-o\n",
      "1.3B\n",
      "0.09\n",
      "11.47\n",
      "2.57\n",
      "+ DAiD (ours)\n",
      "1.3B\n",
      "0.08\n",
      "9.28\n",
      "2.49\n",
      "+ FAiD (ours)\n",
      "1.5B\n",
      "0.06\n",
      "7.93\n",
      "1.73\n",
      "Janus-Pro\n",
      "1.0B\n",
      "0.12\n",
      "14.33\n",
      "28.41\n",
      "+ RA-CM3\n",
      "1.0B\n",
      "0.13\n",
      "12.40\n",
      "20.57\n",
      "+ DAiD (ours)\n",
      "1.0B\n",
      "0.11\n",
      "9.15\n",
      "28.00\n",
      "+ FAiD (ours)\n",
      "1.2B\n",
      "0.07\n",
      "6.67\n",
      "9.40\n",
      "Table 3: Evaluation of text-to-image generation\n",
      "ability on the Midjourney-30K benchmark.\n",
      "image-level retrieval augmentation methods strug-\n",
      "gle to retrieve meaningful references when the num-\n",
      "ber of distinct entities and attributes in a prompt\n",
      "increases.\n",
      "In contrast, our autoregressive aug-\n",
      "mentation framework overcomes this limitation by\n",
      "dynamically retrieving patch-level visual features\n",
      "based on the evolving image context rather than\n",
      "the original prompt, enabling more targeted and\n",
      "effective augmentation.\n",
      "On Midjourney-30K, our proposed methods con-\n",
      "sistently outperform both Janus-Pro and Show-o\n",
      "baselines across all three evaluation metrics. No-\n",
      "tably, despite operating locally at the patch level, our approach leads to a significant reduction in FID\n",
      "7\n",
      "scores, indicating improved global visual quality and closer alignment with the distribution of real\n",
      "images. This suggests that context-aware, auto-regressive retrieval and refinement can propagate to\n",
      "enhance holistic image fidelity. Furthermore, the improvements in CMMD and FWD metrics confirm\n",
      "our method’s effectiveness in reducing visual distortions and enhancing coherence. These results also\n",
      "demonstrate that AR-RAG delivers robust and architecture-agnostic improvements, validating its\n",
      "broad applicability across different image generation backbones.\n",
      "5.2\n",
      "Qualitative Analysis\n",
      "A photo of a\n",
      "bench.\n",
      "A realistic portrait\n",
      "of taylor swift\n",
      "with a red scarf.\n",
      "The morning light\n",
      "filters cast a soft\n",
      "glow on a pair of\n",
      "high-top sneakers.\n",
      "A solitary camel\n",
      "slowly ambles\n",
      "beside a plush,\n",
      "round red couch.\n",
      "Janus-Pro\n",
      "DAiD\n",
      "FAiD\n",
      "A photo of a sheep.\n",
      "Figure 4: Qualitative results of DAiD, FAiD and baselines.\n",
      "Figure 4 illustrates these quan-\n",
      "titative improvements with rep-\n",
      "resentative examples from DPG-\n",
      "Bench (left three columns) and\n",
      "GenEval (right two columns).\n",
      "These\n",
      "examples\n",
      "demonstrate\n",
      "how autoregressive retrieval aug-\n",
      "mentation improves the vanilla\n",
      "image generation models. The\n",
      "vanilla model struggles with\n",
      "object interactions (e.g., col-\n",
      "umn 3,\n",
      "where shoes merge\n",
      "with a coffee machine in the\n",
      "background), complex structures\n",
      "(e.g., columns 2 and 5, where\n",
      "camels and sheep have anatom-\n",
      "ically incorrect numbers of or-\n",
      "gans), and implausible configu-\n",
      "rations (e.g., column 4, where a chair exhibits an impossible design). Both DAiD and FAiD\n",
      "substantially reduce such local distortions, with FAiD yielding the highest visual quality. These\n",
      "results confirm that autoregressive retrieval effectively maintains object consistency and structural\n",
      "integrity throughout the generation process, particularly for complex objects and multi-object scenes.\n",
      "(d) A photo of a green couch and an orange umbrella.\n",
      "(c) A photo of a green cup and a yellow bowl.\n",
      "Retrieved Image\n",
      "AR-RAG\n",
      "ImageRAG\n",
      "Retrieved Image\n",
      "AR-RAG\n",
      "ImageRAG\n",
      "(a) A photo of an apple.\n",
      "Retrieved Image\n",
      "AR-RAG\n",
      "ImageRAG\n",
      "Retrieved Image\n",
      "AR-RAG\n",
      "(b) A photo of a white dog and a blue potted plant.\n",
      "ImageRAG\n",
      "Figure 5: Images generated by ImageRAG [33] and our AR-RAG. ImageRAG excessively copies\n",
      "retrieved images and does not follow user prompts.\n",
      "Figure 5 presents a comparative analysis of conventional image-level and our autoregressive patch-\n",
      "level retrieval augmentation methods. By comprehensively examining images produced by Im-\n",
      "ageRAG alongside their corresponding retrieved reference images, we identify two critical challenges\n",
      "inherent in image-level retrieval augmentation approaches. First, these methods tend to overcopy\n",
      "irrelevant visual elements from retrieved reference images into the generation outputs. As illustrated\n",
      "in Figure 5 (a), when generating an image of an apple, image-level retrieval approaches retrieve\n",
      "a reference image showing an apple on a tree branch and subsequently incorporate both the apple\n",
      "and the surrounding branches, despite the prompt making no mention of them. Similarly, for the\n",
      "prompt “a green cup and a yellow bowl” in Figure 5 (b), the image-level retrieval augmentation\n",
      "approach retrieves a green Starbucks cup and reproduces the pattern on the cup in the generated image,\n",
      "8\n",
      "despite this element not being part of the original instruction. This overcopying behavior directly\n",
      "compromises the instruction-following capability of generative models. Figure 5 (c) demonstrates\n",
      "that when prompted to generate “A photo of a white dog and a blue potted plant,” image-level retrieval\n",
      "methods produce an image containing only the white dog, omitting the blue potted plant entirely.\n",
      "Similarly, for “a photo of a green couch and an orange umbrella” in Figure 5 (d), the generated image\n",
      "fails to include the umbrella. This degradation in instruction following occurs because image-level\n",
      "retrieval biases the generation process toward the compositional structure of retrieved reference\n",
      "images, which may not align with the multi-object relationships specified in the prompt. In contrast,\n",
      "by autoregressively retrieving and integrating visual information at the fine-grained patch level rather\n",
      "than the image level, AR-RAG enables selective incorporation of relevant visual elements while\n",
      "maintaining independence from irrelevant contextual features present in the reference images.\n",
      "5.3\n",
      "Inference Time Cost\n",
      "Single GPU (L40)\n",
      "Model\n",
      "Total (s)\n",
      "Average (s)\n",
      "ImageRAG\n",
      "879.64\n",
      "8.80\n",
      "Janus-Pro\n",
      "457.74\n",
      "4.58\n",
      "+ DAiD\n",
      "459.34\n",
      "4.59 (+0.22%)\n",
      "+ FAiD\n",
      "623.01\n",
      "6.23 (+36.03%)\n",
      "Table 4: Inference time for generat-\n",
      "ing 100 images on a single L40 card.\n",
      "Table 4 shows the inference time comparisons across different\n",
      "models when generating 100 images using both a single L40\n",
      "GPU. The DAiD method introduces only a minimal increase\n",
      "in inference time compared to the base Janus-Pro-1B model,\n",
      "with an average overhead of just 0.22%, demonstrating that\n",
      "DAiD maintains high computational efficiency. FAiD shows\n",
      "a more noticeable overhead of 36.03% on a single GPU due\n",
      "to its autoregressive retrieval and feature blending operations.\n",
      "However, this increase remains reasonable given the substan-\n",
      "tial performance gains in generation quality. Overall, both DAiD and FAiD do not significantly\n",
      "compromise the inference efficiency of Janus-Pro, making them practical for real-world applications.\n",
      "6\n",
      "Related Work\n",
      "Retrieval-augmented generation (RAG) has emerged as a powerful paradigm that enhances generative\n",
      "models by incorporating external knowledge during decoding [23, 13, 16, 47, 46, 15, 24, 25, 42].\n",
      "Originally developed for natural language processing, RAG enables models to retrieve relevant\n",
      "documents to supplement parametric knowledge during response generation [4], and has been widely\n",
      "adopted in many downstream tasks, such as knowledge-intensive tasks [23], document fusion [19],\n",
      "model pretraining [16], dialogue generation [35, 1], and so on.\n",
      "Beyond the text domain, prior research has explored enhancing image generation by incorporating\n",
      "external visual references. Early approaches [8, 3] condition the diffusion process on retrieved\n",
      "images, typically encoded via CLIP or VAE encoders, to guide generation toward higher visual\n",
      "fidelity. KNN-Diffusion [34] extends this idea by leveraging k-nearest neighbor images to improve\n",
      "zero-shot generalization to novel domains. Building on this retrieval-augmented framework, more\n",
      "recent methods [49, 33] introduce adaptive retrieval pipelines that iteratively refine retrieved images\n",
      "based on feedback from multimodal large language models (MLLMs) analyzing the generated outputs.\n",
      "These methods enable context-aware and prompt-sensitive guidance during generation. Another line\n",
      "of work [46] encodes multimodal retrievals into discrete visual and text tokens, and uses them directly\n",
      "as contextual input to augment the generation process of a multimodal large language model. All of\n",
      "these works differe from our method by that our method works on patch-level, enabling more fine\n",
      "grain retrievals and can dynamically adjust retrievals based on evolving generation states.\n",
      "7\n",
      "Conclusion\n",
      "In this work, we propose Autoregressive Retrieval Augmentation (AR-RAG), a novel retrieval\n",
      "paradigm that enhances image synthesis by leveraging k-nearest neighbor retrievals at the patch level.\n",
      "Unlike traditional image-level retrieval approaches, AR-RAG enables fine-grained visual element\n",
      "integration while maintaining compositional flexibility. We introduce two parallel frameworks: (1)\n",
      "Distribution-Augmentation in Decoding (DAiD), a training-free approach that integrates retrieved\n",
      "patch distributions directly into generation, and (2) Feature-Augmentation in Decoding (FAiD), which\n",
      "employs parameter-efficient fine-tuning with multi-scale feature smoothing and compatibility-based\n",
      "feature augmentation. Extensive experiments across GenEval, DPG-Bench, and Midjourney-30K\n",
      "demonstrate that AR-RAG significantly outperforms both conventional and retrieval-augmented\n",
      "baselines, particularly in handling complex prompts with multiple objects and specific spatial rela-\n",
      "9\n",
      "tionships. Our methods substantially reduce local distortions in generated images, improving object\n",
      "consistency and structural integrity.\n",
      "References\n",
      "[1] Trevor Ashby, Adithya Kulkarni, Jingyuan Qi, Minqian Liu, Eunah Cho, Vaibhav Kumar, and\n",
      "Lifu Huang. Towards effective long conversation generation with dynamic topic tracking and\n",
      "recommendation. In Proceedings of the 17th International Natural Language Generation\n",
      "Conference, pages 540–556, 2024.\n",
      "[2] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\n",
      "Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models\n",
      "with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n",
      "[3] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Semi-\n",
      "parametric neural image synthesis. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\n",
      "Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n",
      "[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\n",
      "Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\n",
      "Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang,\n",
      "Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving,\n",
      "Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre.\n",
      "Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri,\n",
      "Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings\n",
      "of the 39th International Conference on Machine Learning, volume 162 of Proceedings of\n",
      "Machine Learning Research, pages 2206–2240. PMLR, 17–23 Jul 2022.\n",
      "[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\n",
      "web-scale image-text pre-training to recognize long-tail visual concepts. CoRR, abs/2102.08981,\n",
      "2021.\n",
      "[6] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi\n",
      "Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: A family of\n",
      "fully open unified multimodal models-architecture, training and dataset, 2025.\n",
      "[7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang,\n",
      "James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion\n",
      "transformer for photorealistic text-to-image synthesis. CoRR, abs/2310.00426, 2023.\n",
      "[8] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. Re-imagen: Retrieval-\n",
      "augmented text-to-image generator. In The Eleventh International Conference on Learning\n",
      "Representations, 2023.\n",
      "[9] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu,\n",
      "and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and\n",
      "model scaling. CoRR, abs/2501.17811, 2025.\n",
      "[10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\n",
      "Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion\n",
      "English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image\n",
      "synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna,\n",
      "Austria, July 21-27, 2024. OpenReview.net, 2024.\n",
      "[11] Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank\n",
      "Wang.\n",
      "Frido: Feature pyramid diffusion for complex scene image synthesis.\n",
      "CoRR,\n",
      "abs/2208.13753, 2022.\n",
      "[12] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\n",
      "Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim\n",
      "Entezari, Giannis Daras, Sarah M Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,\n",
      "Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga\n",
      "10\n",
      "Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\n",
      "Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.\n",
      "Datacomp: In search of the next generation of multimodal datasets. In Thirty-seventh Conference\n",
      "on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\n",
      "[13] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\n",
      "Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large\n",
      "language models: A survey. CoRR, abs/2312.10997, 2023.\n",
      "[14] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework\n",
      "for evaluating text-to-image alignment. CoRR, abs/2310.11513, 2023.\n",
      "[15] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng\n",
      "Gao. Kat: A knowledge augmented transformer for vision-and-language. arXiv preprint\n",
      "arXiv:2112.08614, 2021.\n",
      "[16] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval\n",
      "augmented language model pre-training. In Proceedings of the 37th International Conference\n",
      "on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings\n",
      "of Machine Learning Research, pages 3929–3938. PMLR, 2020.\n",
      "[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n",
      "Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings\n",
      "of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page\n",
      "6629–6640, Red Hook, NY, USA, 2017. Curran Associates Inc.\n",
      "[18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion\n",
      "models with llm for enhanced semantic alignment, 2024.\n",
      "[19] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\n",
      "for open domain question answering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty,\n",
      "editors, Proceedings of the 16th Conference of the European Chapter of the Association for\n",
      "Computational Linguistics: Main Volume, pages 874–880, Online, April 2021. Association for\n",
      "Computational Linguistics.\n",
      "[20] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti,\n",
      "and Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric for image generation. In\n",
      "CVPR, pages 9307–9315, 2024.\n",
      "[21] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs.\n",
      "IEEE Transactions on Big Data, 7(3):535–547, 2019.\n",
      "[22] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: A\n",
      "native skeleton-guided diffusion model for human image generation. CoRR, abs/2304.04269,\n",
      "2023.\n",
      "[23] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo\n",
      "Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\n",
      "editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\n",
      "Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n",
      "[24] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained late-\n",
      "interaction multi-modal retrieval for retrieval augmented visual question answering. Advances\n",
      "in Neural Information Processing Systems, 36:22820–22840, 2023.\n",
      "[25] Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. Preflmr: Scaling up fine-grained\n",
      "late-interaction multi-modal retrievers. arXiv preprint arXiv:2402.08327, 2024.\n",
      "[26] Wenquan Lu, Yufei Xu, Jing Zhang, Chaoyue Wang, and Dacheng Tao. Handrefiner: Refining\n",
      "malformed hands in generated images by diffusion-based conditional inpainting. In Jianfei Cai,\n",
      "Mohan S. Kankanhalli, Balakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian,\n",
      "11\n",
      "Liang Zheng, Vivek K. Singh, Pablo César, Lexing Xie, and Dong Xu, editors, Proceedings of\n",
      "the 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia,\n",
      "28 October 2024 - 1 November 2024, pages 7085–7093. ACM, 2024.\n",
      "[27] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang\n",
      "Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer\n",
      "between modalities with metaqueries. CoRR, abs/2504.06256, 2025.\n",
      "[28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller,\n",
      "Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution\n",
      "image synthesis. In The Twelfth International Conference on Learning Representations, ICLR\n",
      "2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.\n",
      "[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. pages 8748–8763, 2021.\n",
      "[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\n",
      "text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n",
      "[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\n",
      "High-resolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021.\n",
      "[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June\n",
      "2022.\n",
      "[33] Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, and Ohad Fried. Imagerag: Dynamic\n",
      "image retrieval for reference-guided image generation, 2025.\n",
      "[34] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and\n",
      "Yaniv Taigman. kNN-diffusion: Image generation via large-scale retrieval. In The Eleventh\n",
      "International Conference on Learning Representations, 2023.\n",
      "[35] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval aug-\n",
      "mentation reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang,\n",
      "Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2021, pages 3784–3803, Punta Cana, Dominican Republic, November\n",
      "2021. Association for Computational Linguistics.\n",
      "[36] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang,\n",
      "Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li.\n",
      "Journeydb: A benchmark for generative image understanding. In Alice Oh, Tristan Naumann,\n",
      "Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural\n",
      "Information Processing Systems 36: Annual Conference on Neural Information Processing\n",
      "Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\n",
      "[37] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan\n",
      "Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. CoRR,\n",
      "abs/2406.06525, 2024.\n",
      "[38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive\n",
      "modeling: Scalable image generation via next-scale prediction. In A. Globerson, L. Mackey,\n",
      "D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural\n",
      "Information Processing Systems, volume 37, pages 84839–84865. Curran Associates, Inc.,\n",
      "2024.\n",
      "[39] Lokesh Veeramacheneni, Moritz Wolter, Hilde Kuehne, and Juergen Gall. Fréchet wavelet\n",
      "distance: A domain-agnostic metric for image generation. In The Thirteenth International\n",
      "Conference on Learning Representations, 2025.\n",
      "[40] Vivym.\n",
      "Midjourney prompts dataset.\n",
      "https://huggingface.co/datasets/vivym/\n",
      "midjourney-prompts, 2023. Accessed: 2024-04-11.\n",
      "12\n",
      "[41] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan\n",
      "Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya\n",
      "Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu,\n",
      "Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you\n",
      "need. CoRR, abs/2409.18869, 2024.\n",
      "[42] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu\n",
      "Chen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv\n",
      "preprint arXiv:2311.17136, 2023.\n",
      "[43] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang,\n",
      "Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: efficient high-resolution image\n",
      "synthesis with linear diffusion transformers. CoRR, abs/2410.10629, 2024.\n",
      "[44] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong\n",
      "Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single\n",
      "transformer to unify multimodal understanding and generation. CoRR, abs/2408.12528, 2024.\n",
      "[45] Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, and\n",
      "Lifu Huang. Modality-specialized synergizers for interleaved vision-language generalists. In\n",
      "The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\n",
      "April 24-28, 2025. OpenReview.net, 2025.\n",
      "[46] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy\n",
      "Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. Retrieval-augmented multimodal\n",
      "language modeling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\n",
      "Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning,\n",
      "ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine\n",
      "Learning Research, pages 39755–39769. PMLR, 2023.\n",
      "[47] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented\n",
      "language models robust to irrelevant context. CoRR, abs/2310.01558, 2023.\n",
      "[48] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang,\n",
      "Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell\n",
      "Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen\n",
      "Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli\n",
      "Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal\n",
      "models: Pretraining and instruction tuning. CoRR, abs/2309.02591, 2023.\n",
      "[49] Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng\n",
      "Dou. FineRAG: Fine-grained retrieval-augmented text-to-image generation. In Owen Ram-\n",
      "bow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven\n",
      "Schockaert, editors, Proceedings of the 31st International Conference on Computational Lin-\n",
      "guistics, pages 11196–11205, Abu Dhabi, UAE, January 2025. Association for Computational\n",
      "Linguistics.\n",
      "13\n",
      "A\n",
      "Multi-Scale Feature Smoothing Algorithm\n",
      "Algorithm 1: Multi-Scale Feature Smoothing\n",
      "Input: Image Representations Hl ∈R\n",
      "√\n",
      "N×\n",
      "√\n",
      "N×D,\n",
      "Retrieved Patch Representations\n",
      "[ˆh1, ˆh2, . . . , ˆhK], Next Patch Index (i, j)\n",
      "Output: Updated hidden states [ˆh1, ˆh2, . . . , ˆhK]\n",
      "1 foreach ˆhi ∈[ˆh1, . . . , ˆhK] do\n",
      "2\n",
      "for q = 2 to Q do\n",
      "3\n",
      "Initialize tensor: M ←0 ∈RQ×Q×D;\n",
      "4\n",
      "Initialize tensor: ˆhq ←0 ∈RD;\n",
      "5\n",
      "for m = q down to 1 do\n",
      "6\n",
      "for n = q down to 1 do\n",
      "7\n",
      "Hl\n",
      "loc ←Hl[i −m : i + q −m, j −n :\n",
      "j + q −n];\n",
      "8\n",
      "Mmn ←Conv1\n",
      "q×q(Hl\n",
      "loc);\n",
      "9\n",
      "ˆhq += Conv2\n",
      "q×q(M);\n",
      "10\n",
      "ˆhi ←\n",
      "ˆhq\n",
      "Q−1;\n",
      "Algorithm A illustrates the multi-scale fea-\n",
      "ture smoothing, which is the core computa-\n",
      "tional procedure for refining retrieved patch\n",
      "representations within their generation con-\n",
      "text. This algorithm ensures that retrieved\n",
      "visual elements are spatially and stylisti-\n",
      "cally coherent with the surrounding image\n",
      "content through systematic multi-scale con-\n",
      "volution operations.\n",
      "The algorithm processes each retrieved\n",
      "patch representation ˆhi independently, ap-\n",
      "plying convolution operations at multiple\n",
      "scales ranging from 2×2 to Q×Q kernels.\n",
      "For each scale q, the algorithm initializes\n",
      "a temporary feature tensor M ∈RQ×Q×D\n",
      "and an accumulation vector ˆhq ∈RD. The\n",
      "nested loops over indices m and n system-\n",
      "atically extract local patch features from\n",
      "different spatial windows around the target\n",
      "position (i, j). Each extraction operation\n",
      "Hl\n",
      "loc ←Hl[i −m : i + q −m, j −n :\n",
      "j + q −n] captures a local neighborhood\n",
      "of size q × q centered at varying offsets from the target position.\n",
      "The extracted local features undergo two-stage convolution processing. The first convolution operation\n",
      "Conv1\n",
      "q×q transforms the local patch features into an intermediate representation stored in Mmn,\n",
      "effectively capturing contextual relationships within each local window. The second convolution\n",
      "operation Conv2\n",
      "q×q processes the accumulated intermediate features to produce scale-specific refined\n",
      "representations. This two-stage design enables the algorithm to first capture local contextual patterns\n",
      "and then integrate them into a coherent scale-specific feature representation.\n",
      "After processing all scales for a given retrieved patch, the algorithm computes the final refined\n",
      "representation by averaging the scale-specific features. The normalization factor (Q −1) accounts\n",
      "for the number of scales processed, ensuring consistent feature magnitudes across different retrieved\n",
      "patches. This averaging operation effectively combines multi-scale contextual information into a\n",
      "single refined representation that preserves both fine-grained details from smaller kernel sizes and\n",
      "broader contextual patterns from larger kernel sizes. The resulting refined patch representations\n",
      "maintain spatial coherence with the surrounding generation context while preserving the essential\n",
      "visual characteristics of the retrieved content.\n",
      "B\n",
      "Experiment Setup\n",
      "B.1\n",
      "RA-CM3 Implementation Details\n",
      "Since the pretrained RA-CM3 model is not publicly available, we implement our own version\n",
      "following the methodology described in the original paper to serve as a representative baseline for\n",
      "image-level retrieval-augmented generation. Our implementation uses Janus-Pro as the backbone\n",
      "model to ensure fair comparison with our proposed methods, as both approaches operate on the same\n",
      "foundation architecture.\n",
      "We construct an image-level retrieval database using the same CC12M [5] and JourneyDB [36]\n",
      "datasets employed for our patch-level retrieval database to maintain consistency in the underlying\n",
      "data distribution. All images in the database are encoded into 512 dimensional vector representations\n",
      "using a pretrained CLIP [29] model. For each training instance in our 50,000 sample training set, we\n",
      "retrieve the most relevant reference image by encoding the corresponding text prompt with the same\n",
      "CLIP model, extracting the [CLS] token as the text representation, and computing cosine similarity\n",
      "scores between the text representation and all image representations in the database. The image\n",
      "14\n",
      "with the highest similarity score is selected as the retrieved reference. Each retrieved image is then\n",
      "processed through the quantized autoencoder from Janus-Pro to obtain image tokens [v1, . . . , vN] =\n",
      "Z(θEnc(I)), which are subsequently encoded into 2048 dimensional vector representations in the\n",
      "language model’s latent space using the image embedding and aligning layers in Janus-Pro. These\n",
      "retrieved image representations are concatenated with the text embeddings of the input prompts\n",
      "to form the augmented input for training the retrieval-enhanced model, which is the same training\n",
      "strategy used in RA-CM3.\n",
      "During inference, given a text prompt for image generation, we follow the same retrieval process\n",
      "used in training. The input prompt is encoded using the CLIP text encoder, and we compute cosine\n",
      "similarity with all images in the database to identify the most relevant reference image. The retrieved\n",
      "image is processed through the same pipeline to obtain its representation in the language model’s\n",
      "latent space. This representation is then prepended to the text prompt embedding to provide the model\n",
      "with both textual and visual context for generation. The augmented input is fed into the fine-tuned\n",
      "Janus-Pro model to generate the output image following the standard autoregressive generation\n",
      "procedure.\n",
      "B.2\n",
      "Show-o Implementation Details\n",
      "Our patch-based autoregressive retrieval augmentation methods can be theoretically adapted to\n",
      "any model that generates images through discrete tokens. To demonstrate this generalizability,\n",
      "we implement both DAiD and FAiD on the Show-o [44] model, which generates images through\n",
      "a masked token decoding process rather than strict left-to-right autoregression. Show-o decodes\n",
      "multiple image tokens simultaneously at each time step by converting masked tokens to specific image\n",
      "tokens based on a learned probability matrix. This fundamental difference in generation strategy\n",
      "necessitates several architectural adaptations to effectively incorporate our patch-based retrieval\n",
      "mechanisms while maintaining the model’s inherent generation capabilities.\n",
      "DAiD on Show-o\n",
      "The implementation of DAiD on Show-o requires three key modifications to\n",
      "accommodate its non-autoregressive generation strategy. First, instead of constructing retrieval queries\n",
      "from upper-left neighboring patches as in autoregressive models, we utilize all eight surrounding\n",
      "patches to form the h-hop neighborhood representation for each target token position (i, j). This\n",
      "comprehensive neighborhood encoding is computed as [V(i−1)(j−1) : V(i−1)(j) : V(i−1)(j+1) :\n",
      "V(i)(j−1) : V(i)(j+1) : V(i+1)(j−1) : V(i+1)(j) : V(i+1)(j+1)], where missing positions are filled\n",
      "with zero vectors 0. Second, to mitigate retrieval noise arising from sparse neighborhood information\n",
      "in early time steps, we apply patch-level retrieval only during the final half of Show-o’s decoding\n",
      "process when sufficient contextual information is available. Third, since Show-o simultaneously\n",
      "predicts tokens for all patch positions at each time step rather than sequentially, we perform retrieval\n",
      "for all patch positions concurrently. At each qualifying time step t, for every patch position (i, j)\n",
      "in the partially generated image, we extract the eight-neighborhood representation as the retrieval\n",
      "query and obtain the top-K most similar patches [ˆv(i,j)\n",
      "1\n",
      ", ˆv(i,j)\n",
      "2\n",
      ", ..., ˆv(i,j)\n",
      "K\n",
      "] from our database. We\n",
      "then construct position-specific retrieval distributions D(i,j)\n",
      "retrieval ∈R|Z| using the same softmax\n",
      "formulation over retrieval distances as described in the main paper. These retrieval distributions are\n",
      "merged with Show-o’s predicted distributions for each patch position using the weighted average\n",
      "D(i,j)\n",
      "merge = (1 −λ) · D(i,j)\n",
      "model + λ · D(i,j)\n",
      "retrieval, where λ controls the retrieval influence across all positions.\n",
      "FAiD on Show-o\n",
      "The adaptation of FAiD to Show-o involves both training and inference modifica-\n",
      "tions to accommodate the model’s masked token generation process. During training, we prepare\n",
      "the training dataset by applying Show-o’s noise injection process to generate intermediate noisy\n",
      "representations at each time step, which serve as ground truth targets for the denoising process. For\n",
      "each training instance, we save these intermediate representations and apply patch-level retrieval\n",
      "to obtain relevant patches for all time steps. The training objective remains consistent with the\n",
      "standard Show-o formulation, but with augmented input representations that incorporate retrieved\n",
      "patch information. We insert FAiD modules into every L/b decoder layers of Show-o’s Φ model,\n",
      "where each module processes all patch positions simultaneously rather than focusing on a single next\n",
      "token. At each qualifying time step and for each FAiD-equipped layer l, we construct the 2D spatial\n",
      "representation Hl ∈R\n",
      "√\n",
      "N×\n",
      "√\n",
      "N×D from the current hidden states and perform multi-scale feature\n",
      "smoothing for all patch positions. For each position (i, j) and its corresponding retrieved patches\n",
      "15\n",
      "[ˆh(i,j)\n",
      "1\n",
      ", ˆh(i,j)\n",
      "2\n",
      ", ..., ˆh(i,j)\n",
      "K\n",
      "], we apply the convolution operations {Conv2×2, Conv3×3, ..., ConvQ×Q}\n",
      "to capture contextual patterns at multiple scales. The refined representations are computed as\n",
      "ˆh(i,j)\n",
      "k\n",
      "←PQ\n",
      "q=2 softmax(Ω)q · ˆh(i,j)\n",
      "k,q , where ˆh(i,j)\n",
      "k,q\n",
      "represents the output of the q × q convolution\n",
      "for patch k at position (i, j). The final augmented representation for each position is calculated as\n",
      "h(l+1)\n",
      "ij\n",
      "= hl\n",
      "ij + ∆hl\n",
      "ij + PK\n",
      "k=1 s(i,j)\n",
      "k\n",
      "ˆh(i,j)\n",
      "k\n",
      ", where ∆hl\n",
      "ij represents the standard transformer layer\n",
      "updates including self-attention and feed-forward components, and s(i,j)\n",
      "k\n",
      "are position-specific com-\n",
      "patibility scores computed through learned linear projections. During inference, we follow the same\n",
      "procedure but apply retrieval and feature blending only during the final half of the generation time\n",
      "steps to ensure sufficient contextual information is available for effective patch integration.\n",
      "B.3\n",
      "Training Setup\n",
      "Training Datasets\n",
      "For model training, we utilize two large-scale image-caption datasets:\n",
      "CC12M [5] and Midjourney-v6 4. From the training sets of these datasets, we randomly sam-\n",
      "ple a total of 50, 000 image-caption pairs (25, 000 from each dataset) to fine-tune our model. Each\n",
      "image is encoded into 576 patch features and corresponding image tokens with the same image\n",
      "tokenizer [37] employed in the Janus-Pro model. For each image patch, we further retrieve the top-K\n",
      "image tokens from our retrieval database that exhibit similar neighborhood relationships. Conse-\n",
      "quently, each training instance comprises: (1) a textual image caption that serves as the conditioning\n",
      "input, (2) a sequence of 576 image tokens representing the ground-truth image, where each token is\n",
      "paired with K relevant image tokens retrieved from the database based on similar contextual features.\n",
      "Training Details\n",
      "For the implementation of our FAiD approach, we fine-tune two pre-trained text-\n",
      "to-image generation models using the training dataset of 50K text-image pairs that we constructed.\n",
      "We select Janus-Pro-1B [9] and Show-o [44] as our base models. The fine-tuning process is conducted\n",
      "on 4 NVIDIA A100 (80GB) GPUs with a global batch size of 256 for a single epoch. We utilize the\n",
      "AdamW optimizer without weight decay, incorporating a 10% linear warm-up schedule followed by\n",
      "a constant learning rate of 2e-4.\n",
      "B.4\n",
      "Evaluation Benchmarks and Metrics\n",
      "To comprehensively evaluate our proposed methods, we adopt multiple widely used benchmarks that\n",
      "assess different aspects of image generation quality:\n",
      "• GenEval [14] is a benchmark designed to evaluate models’ ability to understand and generate\n",
      "images based on specific attributes and relationships described in text prompts. It comprises\n",
      "multiple categories such as single object generation, two-object composition, counting, colors,\n",
      "positioning, color attribution and so on. Performance is measured as the percentage of generated\n",
      "images that correctly align with the text descriptions.\n",
      "• DPG-Bench [18] (Detailed Prompt Generation Benchmark) evaluates how well image generation\n",
      "models handle detailed prompts with complex requirements, covering categories such as global\n",
      "image quality, entity generation, attribute accuracy, relationship modeling, and other complex\n",
      "generation tasks. Scores are reported as percentages.\n",
      "• For the Midjourney-30k benchmark [40], we employ three complementary metrics to evaluate the\n",
      "quality of generated images, including (1) Fréchet Inception Distance (FID) [17], which measures\n",
      "the statistical similarity between the distributions of generated and real images in the feature space\n",
      "of a pre-trained Inception network; (2) CLIP-MMD (CMMD) [20], which measures the distance\n",
      "between real and generated images using CLIP embeddings and the Maximum Mean Discrepancy,\n",
      "and is specifically designed to better align with human perception of image quality and addresses\n",
      "several limitations of FID, including poor sample efficiency and incorrect normality assumptions;\n",
      "and (3) Fréchet Wavelet Distance (FWD) [39], which measures the distance between real and\n",
      "generated images in the wavelet packet coefficient space. FWD captures both spatial and frequency\n",
      "information without relying on pre-trained networks, making it domain-agnostic and robust to\n",
      "domain shifts across various image types. For all three metrics, lower scores indicate higher-quality\n",
      "image generation, with both CMMD and FWD particularly effective in capturing distortions in\n",
      "generated images in ways that better correlate with human judgements.\n",
      "4https://huggingface.co/datasets/brivangl/midjourney-v6-llava\n",
      "16\n",
      "C\n",
      "Experiment Results and Discussion\n",
      "C.1\n",
      "Accuracy of Patch-based Autoregressive Retrieval\n",
      "Figure 6: l2 distance between ground-truth tokens\n",
      "and top-10 retrieved tokens (blue line) compared\n",
      "to randomly sampled tokens (red dashed line). The\n",
      "curved arrow indicates a broken y-axis that accom-\n",
      "modates the large gap between the retrieved token\n",
      "and the random token baseline.\n",
      "To assess the effectiveness of our patch-level\n",
      "autoregressive retrieval mechanism, we conduct\n",
      "a comparative analysis between the top-K re-\n",
      "trieved image tokens and the ground-truth to-\n",
      "kens to be generated. Specifically, we randomly\n",
      "sampled 1, 000 instances from our training set,\n",
      "each comprising 576 image tokens and 576 × k\n",
      "retrieved tokens. To demonstrate the accuracy\n",
      "of the retrieved image tokens, for each ground-\n",
      "truth image token, we also randomly sample\n",
      "a vocabulary code as non-relevant tokens. Us-\n",
      "ing the shared codebook, we transform all im-\n",
      "age tokens into vector representations and com-\n",
      "pute the l2 distances between each ground-truth\n",
      "image token and its top-K retrieved counter-\n",
      "parts. Similarly, we also compute the mean of\n",
      "the l2 distance between each ground-truth token\n",
      "and the randomly sampled tokens. As shown\n",
      "in Figure 6, the l2 distance between retrieved\n",
      "tokens and ground-truth image tokens is signif-\n",
      "icantly smaller than the distance between ran-\n",
      "domly sampled tokens and ground-truth tokens.\n",
      "As k increases, the distance between the k-th\n",
      "retrieved token and the ground-truth token also increases, demonstrating the effectiveness of the\n",
      "retrieval approach and our assumption that image patches with similar neighbors usually exhibit\n",
      "inherent similarities.\n",
      "C.2\n",
      "Hyperparameter Optimization\n",
      "Figure 7: Hyperparameter optimization results for DAiD and FAiD on FID scores. Left: FID\n",
      "scores for DAiD across different combinations of retrieval temperature τ and merging weight λ.\n",
      "Right: FID scores for FAiD across varying levels of hop h and numbers of blender modules b. All\n",
      "experiments conducted on the Midjourney-10K benchmark, with optimal configurations highlighted\n",
      "by red borders.\n",
      "Both DAiD and FAiD require careful optimization of distinct sets of hyperparameters. For DAiD,\n",
      "we optimize the retrieval temperature τ and merging weight λ, which control the retrieval-based\n",
      "probability distribution sharpness and the balance between retrieval and model predictions, respec-\n",
      "tively. For FAiD, we focus on the level of hop (h) and number of blender modules (b), determining\n",
      "the spatial context incorporated during retrieval and extent of feature blending. To identify optimal\n",
      "17\n",
      "configurations, we conducted a systematic ablation study on the Midjourney-10K benchmark using\n",
      "Fréchet Inception Distance (FID) as the performance metric.\n",
      "Figure 7 presents the FID scores for DAiD across different combinations of λ and τ, and for FAiD\n",
      "across varying levels of (h) and (b), where composite hop levels such as “12” indicate combined\n",
      "use of multiple hop distances. Analysis of the DAiD results reveals that performance degrades\n",
      "as λ increases, suggesting that modest integration of retrieval information enhances performance\n",
      "while excessive reliance impairs generative flexibility. The retrieval temperature τ demonstrates\n",
      "less pronounced effects, though a moderate value of 0.6 provides marginal benefits. For FAiD,\n",
      "configurations incorporating multiple hop levels generally outperform single hop levels, with the “12”\n",
      "configuration yielding optimal results. Regarding blender modules, an intermediate value consistently\n",
      "delivers the best performance, implying that moderate feature blending optimizes the incorporation of\n",
      "retrieved patches while avoiding both under-utilization and over-smoothing. Based on this analysis,\n",
      "we selected λ = 0.05) and τ = 0.6 for DAiD, and hop levels “12” with 2 blender modules for FAiD,\n",
      "achieving FID scores of 14.12 and 13.13, respectively. These configurations effectively harness\n",
      "retrieval information while preserving the generative strengths of the underlying Janus-Pro model, as\n",
      "demonstrated by their superior performance on the benchmark.\n",
      "D\n",
      "Limitations\n",
      "While our AR-RAG framework demonstrates strong performance across multiple benchmarks,\n",
      "several limitations should be acknowledged. First, our approach relies on discrete image tokenization\n",
      "and targets discrete token-based models, so it may not be directly applied to continuous diffusion\n",
      "models operating in latent spaces. Second, due to computational resource limitations, our retrieval\n",
      "database remains smaller than billion-scale databases. This limitation may introduce visual pattern\n",
      "biases, as the database may not fully capture the diversity of real-world visual patterns, potentially\n",
      "affecting the generation of underrepresented visual elements. Third, our implementation focuses\n",
      "exclusively on 2D image generation. While the underlying patch-based retrieval concept could\n",
      "theoretically extend to other structured generation tasks such as 3D point cloud generation, we have\n",
      "not explored these applications.\n",
      "E\n",
      "Broader impacts\n",
      "We propose a novel retrieval-augmented approach to enhance existing image generation models.\n",
      "Our method is both highly efficient and readily adaptable to a wide range of applications, making it\n",
      "valuable for both academic research and industrial deployment. However, as our approach builds upon\n",
      "existing generative models, it may inherit their biases and could potentially produce inappropriate\n",
      "outputs in the absence of additional safety mechanisms. Furthermore, the large-scale retrieval\n",
      "database may contain unsafe or undesirable content, which can be reflected in the retrieved image\n",
      "patches. To ensure safe deployment in real-world scenarios, additional safeguards and filtering\n",
      "measures are necessary to mitigate these risks.\n",
      "18\n",
      "\n",
      "### Question:\n",
      "What is RAG?\n",
      "Use the following pieces of context to answer the question at the end.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate\n",
    "\n",
    "template = \"\"\" You are a research assistant. \n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "### Question:\n",
    "{question}\n",
    "Use the following pieces of context to answer the question at the end.\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=template\n",
    ")\n",
    "print(prompt.format(context=docs1.page_content, question=\"What is RAG?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b84585",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f7c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
